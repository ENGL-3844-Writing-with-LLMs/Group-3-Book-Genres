{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mplcyberpunk\n",
    "# ML Modeling\n",
    "from sklearn.metrics import precision_recall_fscore_support,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import RocCurveDisplay,roc_curve\n",
    "from sklearn.preprocessing import normalize\n",
    "# Saving and importing trained models\n",
    "import pickle\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/data.csv', delimiter=',', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4657, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doing a quick review of the data before developing the model. It looks like we have 4,657 books, and 4 collumns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       Drowned Wednesday\n",
       "1                           The Lost Hero\n",
       "2               The Eyes of the Overworld\n",
       "3                         Magic's Promise\n",
       "4                          Taran Wanderer\n",
       "                      ...                \n",
       "4652                              Hounded\n",
       "4653    Charlie and the Chocolate Factory\n",
       "4654                           Red Rising\n",
       "4655                            Frostbite\n",
       "4656                             Radiance\n",
       "Name: title, Length: 4657, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'title', 'genre', 'summary'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4657 entries, 0 to 4656\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   index      4657 non-null   int64 \n",
      " 1   title      4657 non-null   object\n",
      " 2   genre      4657 non-null   object\n",
      " 3   summary    4657 non-null   object\n",
      " 4   sum_title  4657 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 182.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#processing the data for classification\n",
    "\n",
    "#summaru + title\n",
    "df['sum_title'] = df['summary'] + ' '+ df['title']\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we beging training our model\n",
    "def _reciprocal_rank(true_genre_labels: list, machine_predicted_genre_labels: list):\n",
    "  #determining recirprical rank at cutoff\n",
    "#now wer creaate parameters\n",
    "        # `true_genre_labels` (List): # List of the actual book genre labels\n",
    "        # `machine_predicted_genre_labels` (List): # List of book genre labels predicted by the LR algorithm\n",
    "    \n",
    "    #The return value will be reciprical rank\n",
    "    \n",
    " # add index to list ONLY if machine predicted label is true \n",
    "    tp_pos_list = [(idx + 1) for idx, r in enumerate(machine_predicted_genre_labels) if r in true_genre_labels]\n",
    "\n",
    "    recip_rank = 0\n",
    "    if len(tp_pos_list) > 0:\n",
    "        # finds fist corectly predicted item\n",
    "        first_pos_list = tp_pos_list[0]\n",
    "        \n",
    "    \n",
    "        recip_rank = 1 / float(first_pos_list)\n",
    "\n",
    "    return recip_rank\n",
    "\n",
    "def compute_mrr_at_k(eval_news_category_items:list):\n",
    "    \n",
    "    ## creating a function that computes Mean reciprical rank\n",
    "\n",
    "    rr_total = 0\n",
    "    \n",
    "    for item in eval_news_category_items:\n",
    "        actual_label = item[0]\n",
    "        pred_label_list = item[1]\n",
    "\n",
    "        # Finds the reciprocal rank  for this row\n",
    "        rr_at_k = _reciprocal_rank(actual_label, pred_label_list)\n",
    "\n",
    "        # Add the row's RR to  scores for the entire data\n",
    "        rr_total = rr_total + rr_at_k\n",
    "\n",
    "        # Updates the Mean Reciprocal Rank  score with new row value\n",
    "        mean_reciprocal_rank_score = rr_total / 1/float(len(eval_news_category_items))\n",
    "\n",
    "    return mean_reciprocal_rank_score\n",
    "\n",
    "def collect_preds(Y_test, Y_preds):\n",
    "    ##This function will gather all predicted book genres and the true book genres \n",
    "    pred_gold_list = [ [ [Y_test[index]], pred ] for index, pred in enumerate(Y_preds) ]\n",
    "    return pred_gold_list\n",
    "             \n",
    "def compute_accuracy(eval_book_genre_items:list):\n",
    "    \n",
    "    #this will compute the overall accuracy score our the model \n",
    "    correct_book_cat = 0\n",
    "    \n",
    "    for book_genre_cat in eval_book_genre_items:\n",
    "        true_gen = book_genre_cat[0]\n",
    "        machine_gen = set(book_genre_cat[1])\n",
    "        \n",
    "        for book_cat in true_gen:\n",
    "            if book_cat in machine_gen:\n",
    "                correct_book_cat += 1\n",
    "                break\n",
    "    \n",
    "    book_cat_prediction_accuracy = correct_book_cat / float(len(eval_book_genre_items))\n",
    "    return book_cat_prediction_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "def extract_features(df, field, training_data, testing_data, type='binary'):\n",
    "# this function will extract features using different method types: binary, counts, or TF-IDF\n",
    "\n",
    "    logging.info()\n",
    "    \n",
    "    if 'binary' in type:\n",
    "        \n",
    "        # Now we are creating a new CountVectorizer()\n",
    "        cv = CountVectorizer(binary=True, max_df=0.95)\n",
    "        cv.fit_transform(training_data[field].values)\n",
    "        train_feature_set = cv.transform(training_data[field].values)\n",
    "        test_feature_set = cv.transform(testing_data[field].values)\n",
    "        \n",
    "        return train_feature_set,test_feature_set,cv\n",
    "  \n",
    "    elif 'counts' in type:\n",
    "        \n",
    "        cv = CountVectorizer(binary=False, max_df=0.95)\n",
    "        cv.fit_transform(training_data[field].values)\n",
    "        \n",
    "        train_feature_set = cv.transform(training_data[field].values)\n",
    "        test_feature_set = cv.transform(testing_data[field].values)\n",
    "        \n",
    "        return train_feature_set,test_feature_set,cv\n",
    "    \n",
    "    elif 'tfidf':    \n",
    "        \n",
    "        tfidf_vectorizer=TfidfVectorizer(use_idf=True, max_df=0.95)\n",
    "        tfidf_vectorizer.fit_transform(training_data[field].values)\n",
    "        \n",
    "        train_feature_set=tfidf_vectorizer.transform(training_data[field].values)\n",
    "        test_feature_set=tfidf_vectorizer.transform(testing_data[field].values)\n",
    "        \n",
    "        return train_feature_set,test_feature_set,tfidf_vectorizer\n",
    "\n",
    "def get_top_b_predictions(model, X_test, k, threshold=False):\n",
    " # this will use our input to return the book genre with the top estimated probability of being accurate\n",
    "    if threshold == False:\n",
    "        probs = model.predict_proba(X_test)\n",
    "        best_n = np.argsort(probs, axis=1)[:,-k:]\n",
    "        preds = [[model.classes_[predicted_cat] for predicted_cat in prediction] for prediction in best_n]\n",
    "        preds = [ item[::-1] for item in preds]\n",
    "    \n",
    "        return preds\n",
    "    else:\n",
    "        # get probabilities instead of predicted labels\n",
    "        probs = (model.predict_proba(X_test)[:,1] >= thresh_val)\n",
    "\n",
    "        # top predictions by probability\n",
    "        best_n = np.argsort(probs, axis=1)[:,-k:]\n",
    "        \n",
    "        # gets category of predictions\n",
    "        preds = [[model.classes_[predicted_cat] for predicted_cat in prediction] for prediction in best_n]\n",
    "        \n",
    "        preds = [ item[::-1] for item in preds]\n",
    "    \n",
    "        return preds\n",
    "   \n",
    "def train_model(df, field=\"summary\", feature_rep=\"binary\", top_k=3):\n",
    " # now we are creating the main control of the function\n",
    "\n",
    "    y = df['genre']\n",
    "    x_training_data,x_testing_data = train_test_split(\n",
    "        df,\n",
    "        random_state=2000  )\n",
    "\n",
    "    # getting labels and category values from each split data\n",
    "    Y_train = x_training_data['genre'].values\n",
    "    Y_test = x_testing_data['genre'].values\n",
    "     \n",
    "    # Extracting features\n",
    "    X_train,X_test,feature_transformer = extract_features(\n",
    "        df,\n",
    "        field,\n",
    "        x_training_data,\n",
    "        x_testing_data,\n",
    "        type=feature_rep\n",
    "    )\n",
    "\n",
    "    # start classifier object\n",
    "    logging.info(\"Training a Logistic Regression Model. This may take a few minutes. ...\")\n",
    "    scikit_log_reg = LogisticRegression(\n",
    "        verbose=0, \n",
    "        solver='liblinear',\n",
    "        random_state=0,\n",
    "        C=5,\n",
    "        penalty='l2',\n",
    "        max_iter=1000\n",
    "    )\n",
    "    # Create the model\n",
    "    model = scikit_log_reg.fit(X_train, Y_train)\n",
    "\n",
    "    # top predictions\n",
    "    preds = get_top_k_predictions(model, X_test, top_k)\n",
    "    \n",
    "    eval_items = collect_preds(Y_test, preds)\n",
    "    \n",
    "    # Evaluation on test dat\n",
    "    logging.info(\"Starting evaluation...\")\n",
    "    simple_mean_avg_correct_prediction_accuracy = compute_accuracy(eval_items)\n",
    "    mean_recip_rank_at_k = compute_mrr_at_k(eval_items)\n",
    "    \n",
    "    logging.info(\"Done training and evaluation.\")\n",
    "\n",
    "    # Return the computed model \n",
    "    return model,feature_transformer,simple_mean_avg_correct_prediction_accuracy,mean_recip_rank_at_k,X_train,X_test,Y_test,Y_train,preds,eval_items"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
